{"metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 3}, "pygments_lexer": "python3"}, "qubole": {"kernel_log_url": "https://us.qubole.com/jupyter-notebook-49246/qubole/api/v1/kernel_logs/76e9498b-2ff8-4c2c-8a09-cd988b7461d1"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "code", "source": "import numpy as np\nfrom pyspark.sql import SQLContext\nfrom pyspark.conf import SparkConf\nfrom pyspark.sql import SparkSession\nimport pandas as pd\n# import pandas.Series.tolist\nimport pandasql as ps\nimport boto3\nimport pyodbc\nimport io\nfrom datetime import date,timedelta\nfrom IPython.display import display\nimport json\nimport random\nimport time\nimport re\n### Custom Lib\npd.options.mode.chained_assignment = None\nfrom pyspark.sql.types import IntegerType,StructType,StructField,StringType\nfrom pyspark.sql.functions import lit,array, create_map, struct\nfrom pyspark.sql.functions import col, concat, regexp_extract, when,max as max_,min as min_\nfrom pyspark.sql.functions import date_format\nfrom pyspark.sql.functions import udf,col\nfrom datetime import datetime,timedelta\nfrom pymongo import MongoClient\nimport traceback\nimport ftplib\n\n", "metadata": {"qubole": {"import_errors": true, "execution_info": {"started_at": "2024-05-22T09:13:11.771342Z", "ended_at": "2024-05-22T09:13:11.776114Z", "user_email": "agaur@affinitiv.com", "cluster": "ADMT_API_PROD_CLUSTER", "clusterType": "cluster"}}, "trusted": true}, "execution_count": 4, "outputs": [{"name": "stderr", "text": "An error was encountered:\nYou need to have at least 1 client created to execute commands.\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "def errorcategorycode(errtext):\n    cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=10.100.11.5;DATABASE=datalake;UID=datalake_prod;PWD=uqkHej49KENG')\n    querystring_details = \"select * from etl.error_category with(nolock)\"\n    error_code='MISC'\n    try:\n        dataframe_details = pd.read_sql(querystring_details, cnxn)\n        if len(dataframe_details) > 0:\n            for row in range(0,len(dataframe_details)):\n                error_keywords = dataframe_details['error_category_text'][row].split(\"|\")\n                if any(x in errtext for x in error_keywords):\n                    error_code = dataframe_details['error_category_code'][row]\n                    break\n    except:\n        pass\n    cnxn.close()\n    return error_code", "metadata": {"qubole": {"execution_info": {"started_at": "2024-05-22T09:14:28.778186Z", "ended_at": "2024-05-22T09:14:28.790063Z", "user_email": "agaur@affinitiv.com", "cluster": "ADMT_API_PROD_CLUSTER", "clusterType": "cluster"}}, "trusted": true}, "execution_count": 5, "outputs": [{"name": "stderr", "text": "An error was encountered:\nYou need to have at least 1 client created to execute commands.\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "sc.addPyFile(\"s3n://c1-data-lake/ETL/ETL_Dependency/etlconfig.py\")\nsc.addPyFile(\"s3://c1-data-lake/ETL/UDF/prod/Udf.py\")\nsc.addPyFile(\"s3n://c1-data-lake/ETL/psqlconfig/psqlconfig.py\")\n\nimport Udf\nimport etlconfig\nimport psqlconfig\n\nconf = SparkConf()\n\nconf.set(\"spark.serializer\", \"org.apache.spark.serializer.KyroSerializer\")\nconf.set(\"spark.executor.extraJavaOptions\", \"-XX:+UseParallelGC -XX:+UseParallelOldGC\")\nconf.set(\"spark.jars\", \"s3://c1-data-lake/jars/postgresql-42.3.1.jar\")\nconf.set('spark.executor.extraClassPath', \"s3://c1-data-lake/jars/postgresql-42.3.1.jar\")\nconf.set('spark.driver.extraClassPath', 's3://c1-data-lake/jars/postgresql-42.3.1.jar')\n\n\nspark = SparkSession.builder\\\n                    .appName(\"Sparkonlineclient\") \\\n                    .config(conf=conf) \\\n                    .getOrCreate()\n\n\nsc=spark.sparkContext\nsqlCtx = SQLContext(sc)\nsqlCtx.clearCache()\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\nsparkread = spark.read\ntodaydate = date.today()\ndate_formatsmall=todaydate.strftime(\"%y%m%d\")\n# todaydate = date.today()\ntodaydate = date.today() - timedelta(days=1)\nprint(todaydate)\ncount=0\nbucket='c1-data-lake'\ns3_resource = boto3.client('s3')\n", "metadata": {}, "execution_count": null, "outputs": []}]}