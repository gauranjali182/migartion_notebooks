{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"qubole":{"execution_info":{"cluster":"ADMT_API_PROD_CLUSTER","clusterType":"cluster","ended_at":"2024-05-22T09:13:11.776114Z","started_at":"2024-05-22T09:13:11.771342Z","user_email":"agaur@affinitiv.com"},"import_errors":true},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["An error was encountered:\n","You need to have at least 1 client created to execute commands.\n"]}],"source":["\n","#Hi anjali this side\n","import numpy as np\n","from pyspark.sql import SQLContext\n","from pyspark.conf import SparkConf\n","from pyspark.sql import SparkSession\n","import pandas as pd\n","# import pandas.Series.tolist\n","import pandasql as ps\n","import boto3\n","import pyodbc\n","import io\n","from datetime import date,timedelta\n","from IPython.display import display\n","import json\n","import random\n","import time\n","import re\n","### Custom Lib\n","pd.options.mode.chained_assignment = None\n","from pyspark.sql.types import IntegerType,StructType,StructField,StringType\n","from pyspark.sql.functions import lit,array, create_map, struct\n","from pyspark.sql.functions import col, concat, regexp_extract, when,max as max_,min as min_\n","from pyspark.sql.functions import date_format\n","from pyspark.sql.functions import udf,col\n","from datetime import datetime,timedelta\n","from pymongo import MongoClient\n","import traceback\n","import ftplib\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"qubole":{"execution_info":{"cluster":"ADMT_API_PROD_CLUSTER","clusterType":"cluster","ended_at":"2024-05-22T09:14:28.790063Z","started_at":"2024-05-22T09:14:28.778186Z","user_email":"agaur@affinitiv.com"}},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["An error was encountered:\n","You need to have at least 1 client created to execute commands.\n"]}],"source":["def errorcategorycode(errtext):\n","    cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=10.100.11.5;DATABASE=datalake;UID=datalake_prod;PWD=uqkHej49KENG')\n","    querystring_details = \"select * from etl.error_category with(nolock)\"\n","    error_code='MISC'\n","    try:\n","        dataframe_details = pd.read_sql(querystring_details, cnxn)\n","        if len(dataframe_details) > 0:\n","            for row in range(0,len(dataframe_details)):\n","                error_keywords = dataframe_details['error_category_text'][row].split(\"|\")\n","                if any(x in errtext for x in error_keywords):\n","                    error_code = dataframe_details['error_category_code'][row]\n","                    break\n","    except:\n","        pass\n","    cnxn.close()\n","    return error_code"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sc.addPyFile(\"s3n://c1-data-lake/ETL/ETL_Dependency/etlconfig.py\")\n","sc.addPyFile(\"s3://c1-data-lake/ETL/UDF/prod/Udf.py\")\n","sc.addPyFile(\"s3n://c1-data-lake/ETL/psqlconfig/psqlconfig.py\")\n","\n","import Udf\n","import etlconfig\n","import psqlconfig\n","\n","conf = SparkConf()\n","\n","conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KyroSerializer\")\n","conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+UseParallelGC -XX:+UseParallelOldGC\")\n","conf.set(\"spark.jars\", \"s3://c1-data-lake/jars/postgresql-42.3.1.jar\")\n","conf.set('spark.executor.extraClassPath', \"s3://c1-data-lake/jars/postgresql-42.3.1.jar\")\n","conf.set('spark.driver.extraClassPath', 's3://c1-data-lake/jars/postgresql-42.3.1.jar')\n","\n","\n","spark = SparkSession.builder\\\n","                    .appName(\"Sparkonlineclient\") \\\n","                    .config(conf=conf) \\\n","                    .getOrCreate()\n","\n","\n","sc=spark.sparkContext\n","sqlCtx = SQLContext(sc)\n","sqlCtx.clearCache()\n","spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n","sparkread = spark.read\n","todaydate = date.today()\n","date_formatsmall=todaydate.strftime(\"%y%m%d\")\n","# todaydate = date.today()\n","todaydate = date.today() - timedelta(days=1)\n","print(todaydate)\n","count=0\n","bucket='c1-data-lake'\n","s3_resource = boto3.client('s3')\n"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"","name":"pysparkkernel"},"language_info":{"codemirror_mode":{"name":"python","version":3},"mimetype":"text/x-python","name":"pyspark","pygments_lexer":"python3"},"qubole":{"kernel_log_url":"https://us.qubole.com/jupyter-notebook-49246/qubole/api/v1/kernel_logs/76e9498b-2ff8-4c2c-8a09-cd988b7461d1"}},"nbformat":4,"nbformat_minor":4}
